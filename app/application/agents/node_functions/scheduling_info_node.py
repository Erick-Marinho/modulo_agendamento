import logging
from typing import Optional

from langchain_core.messages import AIMessage, HumanMessage

from app.application.agents.state.message_agent_state import (
    MessageAgentState,
    SchedulingDetails,
)
from app.infrastructure.services.llm.llm_factory import LLMFactory

logger = logging.getLogger(__name__)


def scheduling_info_node(state: MessageAgentState) -> MessageAgentState:
    """
    NÃ³ responsÃ¡vel por processar informaÃ§Ãµes fornecidas pelo usuÃ¡rio durante o agendamento.
    Este nÃ³ Ã© chamado quando o usuÃ¡rio estÃ¡ respondendo a perguntas sobre agendamento.
    """

    logger.info("--- Executando nÃ³ scheduling_info ---")

    # ðŸ†• DETECÃ‡ÃƒO ESPECÃFICA MELHORADA: Perguntas sobre especialidades/profissionais
    messages = state.get("messages", [])
    last_message = messages[-1].content.lower().strip() if messages else ""

    # ðŸ†• NOVA DETECÃ‡ÃƒO: "NÃ£o sei" apÃ³s lista de profissionais
    # Verificar se o bot mostrou lista de profissionais nas Ãºltimas 2 mensagens
    recent_ai_messages = [msg.content.lower() for msg in messages[-2:] if 'AI' in str(type(msg))]
    showed_professional_list = any(
        ("encontrei os seguintes profissionais" in msg or 
         "para a especialidade" in msg or
         "gostaria de agendar com algum deles" in msg)
        for msg in recent_ai_messages
    )
    
    # Detectar expressÃµes de incerteza
    uncertainty_phrases = [
        "nÃ£o sei", "nao sei", "naÃµ sei",
        "nÃ£o tenho certeza", "nao tenho certeza", 
        "qualquer um", "tanto faz", "qualquer",
        "nÃ£o conheÃ§o", "nao conheÃ§o", "nÃ£o conheco",
        "vocÃª decide", "voce decide",
        "o que vocÃª recomenda", "o que voce recomenda",
        "nÃ£o faÃ§o ideia", "nao faco ideia"
    ]
    
    user_expressed_uncertainty = any(phrase in last_message for phrase in uncertainty_phrases)
    
    # ðŸ†• RESPOSTA ESPECÃFICA: Quando usuÃ¡rio diz "nÃ£o sei" apÃ³s ver lista de profissionais
    if showed_professional_list and user_expressed_uncertainty:
        logger.info(f"ðŸŽ¯ DETECTADO: UsuÃ¡rio expressa incerteza '{last_message}' apÃ³s ver lista de profissionais")
        
        # Buscar qual especialidade foi mostrada no contexto
        extracted_details = state.get("extracted_scheduling_details")
        specialty_name = extracted_details.specialty if extracted_details else "dessa especialidade"
        
        gentle_response = (
            f"Sem problemas! No momento, esses sÃ£o os Ãºnicos profissionais de {specialty_name} "
            f"que temos disponÃ­veis na clÃ­nica.\n\n"
            f"Todos sÃ£o excelentes profissionais e vocÃª pode escolher qualquer um deles. "
            f"Ou, se preferir, posso mostrar profissionais de outra especialidade.\n\n"
            f"O que vocÃª prefere?"
        )
        
        return {
            **state,
            "messages": messages + [AIMessage(content=gentle_response)],
            "next_step": "completed",
            "conversation_context": "awaiting_professional_choice",
        }

    # Palavras-chave expandidas para detectar perguntas sobre API
    api_query_patterns = [
        # PadrÃµes diretos
        "quais especialidades",
        "que especialidades",
        "quais as especialidades",
        "quais sÃ£o as especialidades",
        "especialidades disponÃ­veis",
        "especialidades vocÃªs tem",
        "especialidades tem",
        # Profissionais
        "quais profissionais",
        "que profissionais",
        "quais sÃ£o os profissionais",
        "profissionais disponÃ­veis",
        "profissionais vocÃªs tem",
        "profissionais tem",
        # MÃ©dicos
        "quais mÃ©dicos",
        "que mÃ©dicos",
        "mÃ©dicos disponÃ­veis",
        "mÃ©dicos vocÃªs tem",
        "mÃ©dicos tem",
        # VariaÃ§Ãµes com "tem"
        "tem cardiologista",
        "tem pediatra",
        "tem ortopedista",
        "tem especialista",
        "tem doutor",
        "tem doutora",
        # Comandos de listagem
        "lista de especialidades",
        "lista de profissionais",
        "lista de mÃ©dicos",
        "mostrar especialidades",
        "mostrar profissionais",
        "ver especialidades",
        "ver profissionais",
        # VariaÃ§Ãµes simples
        "especialidades?",
        "profissionais?",
        "mÃ©dicos?",
    ]

    # Detectar se Ã© uma pergunta sobre API
    is_api_query = any(pattern in last_message for pattern in api_query_patterns)

    # DetecÃ§Ã£o adicional: frases que comeÃ§am com palavras interrogativas
    question_words = ["quais", "que", "qual", "tem", "existe", "hÃ¡"]
    medical_terms = ["especialidade", "profissional", "mÃ©dico", "doutor", "doutora"]

    starts_with_question = any(last_message.startswith(word) for word in question_words)
    contains_medical_term = any(term in last_message for term in medical_terms)

    if is_api_query or (starts_with_question and contains_medical_term):
        logger.info(
            f"ðŸŽ¯ DETECTADO: Pergunta sobre especialidades/profissionais: '{last_message}'"
        )
        logger.info("Redirecionando para agent_tool_caller para buscar informaÃ§Ãµes")

        return {
            **state,
            "next_step": "agent_tool_caller",
            "conversation_context": "api_interaction",
        }

    extracted_details = state.get("extracted_scheduling_details")

    if extracted_details is None:
        logger.info("Primeira extraÃ§Ã£o de detalhes de agendamento.")
        return _extract_initial_details(state)

    logger.info("ExtraÃ§Ã£o de detalhes de agendamento jÃ¡ realizada.")
    return _update_existing_details(state)


def _extract_initial_details(state: MessageAgentState) -> MessageAgentState:
    """
    Extrai os detalhes iniciais de agendamento do usuÃ¡rio.
    """
    logger.info("Extraindo detalhes iniciais de agendamento.")

    try:
        llm_service = LLMFactory.create_llm_service("openai")

        messages = state.get("messages", [])
        conversation_history = _format_conversation_history(messages)

        logger.info(f"Extraindo detalhes iniciais do histÃ³rico: {conversation_history}")

        extracted_data = llm_service.extract_scheduling_details(conversation_history)

        if extracted_data:
            logger.info(f"Detalhes iniciais extraÃ­dos: {extracted_data}")
            return {
                **state,
                "extracted_scheduling_details": extracted_data,
                "next_step": "check_completeness",
            }
        else:
            logger.warning("Falha na extraÃ§Ã£o de detalhes de agendamento.")
            return {**state, "next_step": "clarification"}
    except Exception as e:
        logger.error(f"Erro ao extrair detalhes de agendamento: {e}", exc_info=True)
        return {**state, "next_step": "clarification"}


def _update_existing_details(state: MessageAgentState) -> MessageAgentState:
    """
    Atualiza os detalhes de agendamento existentes com as informaÃ§Ãµes fornecidas pelo usuÃ¡rio.
    """
    try:
        conversation_context = state.get("conversation_context", "")

        # ðŸ†• CASO ESPECIAL: UsuÃ¡rio estÃ¡ escolhendo nova data
        if conversation_context == "awaiting_date_selection":
            logger.info(
                "ðŸ”„ Contexto 'awaiting_date_selection' - Processando nova data escolhida"
            )

            # ðŸ”§ CORREÃ‡ÃƒO: Usar histÃ³rico maior para preservar contexto
            llm_service = LLMFactory.create_llm_service("openai")
            all_messages = state.get("messages", [])
            conversation_history = _format_conversation_history(
                all_messages, max_messages=12  # 
            )

            new_details = llm_service.extract_scheduling_details(conversation_history)

            if new_details:
                existing_details = state.get("extracted_scheduling_details")
                updated_details = _merge_scheduling_details(
                    existing_details, new_details
                )

                logger.info(f"Nova data extraÃ­da: {updated_details.date_preference}")

                return {
                    **state,
                    "extracted_scheduling_details": updated_details,
                    "conversation_context": "",
                    "next_step": "check_availability_node",
                }

            logger.warning("Falha ao extrair nova data")
            return {**state, "next_step": "clarification"}

        # Fluxo normal continua...
        llm_service = LLMFactory.create_llm_service("openai")

        all_messages = state.get("messages", [])
        if not all_messages:
            logger.warning("Nenhuma mensagem encontrada para atualizaÃ§Ã£o")
            return {**state, "next_step": "clarification"}

        # ðŸ”§ CORREÃ‡ÃƒO PRINCIPAL: Usar histÃ³rico maior para preservar especialidade
        conversation_history = _format_conversation_history(all_messages, max_messages=10)  # ðŸ”§ AUMENTADO DE 3 PARA 10

        logger.info(
            f"Atualizando detalhes com o histÃ³rico recente: '{conversation_history}'"
        )

        new_details = llm_service.extract_scheduling_details(conversation_history)

        if new_details:
            existing_details = state.get("extracted_scheduling_details")
            updated_details = _merge_scheduling_details(existing_details, new_details)

            logger.info(f"Detalhes atualizados: {updated_details}")

            # --- LÃ“GICA DE DIRECIONAMENTO ---
            if conversation_context == "awaiting_new_date_selection":
                next_node = "check_availability_node"
                logger.info(
                    "Redirecionando de volta para a verificaÃ§Ã£o de disponibilidade."
                )
            else:
                # Comportamento padrÃ£o: verificar se os dados estÃ£o completos.
                next_node = "check_completeness"

            return {
                **state,
                "extracted_scheduling_details": updated_details,
                "next_step": next_node,
            }
        else:
            logger.warning(
                "Nenhum detalhe novo extraÃ­do da mensagem. Verifique o prompt de extraÃ§Ã£o e o contexto."
            )
            return {**state, "next_step": "check_completeness"}

    except Exception as e:
        logger.error(f"Erro ao atualizar detalhes: {e}")
        return {**state, "next_step": "clarification"}


def _format_conversation_history(messages, max_messages: int = 5) -> str:
    """
    Formata o histÃ³rico de conversa para envio ao LLM.
    """
    if not messages:
        return "Nenhuma conversa ainda"

    recent_messages = messages[-max_messages:]
    formatted_history = []

    for msg in recent_messages:
        role = "UsuÃ¡rio" if isinstance(msg, HumanMessage) else "Assistente"
        formatted_history.append(f"{role}: {msg.content}")

    return "\n".join(formatted_history)


def _merge_scheduling_details(existing, new):
    """
    Mescla detalhes de agendamento, priorizando informaÃ§Ãµes mais recentes quando nÃ£o forem None.
    """
    if not existing:
        logger.info("Nenhum detalhe existente, retornando dados novos")
        return new

    if not new:
        logger.info("Nenhum detalhe novo, retornando dados existentes")
        return existing

    # ðŸ”§ LÃ“GICA MELHORADA: Priorizar valores nÃ£o-None ao invÃ©s de sÃ³ usar 'or'
    merged = SchedulingDetails(
        professional_name=(
            new.professional_name 
            if new.professional_name is not None 
            else existing.professional_name
        ),
        specialty=(
            new.specialty 
            if new.specialty is not None 
            else existing.specialty
        ),
        date_preference=(
            new.date_preference 
            if new.date_preference is not None 
            else existing.date_preference
        ),
        time_preference=(
            new.time_preference 
            if new.time_preference is not None 
            else existing.time_preference
        ),
        specific_time=(
            new.specific_time 
            if new.specific_time is not None 
            else existing.specific_time
        ),
        service_type=(
            new.service_type 
            if new.service_type is not None 
            else existing.service_type
        ),
    )

    # ðŸ”§ LOG DETALHADO PARA DEBUG
    logger.info(f"ðŸ”„ MERGE - Existente: {existing}")
    logger.info(f"ðŸ”„ MERGE - Novo: {new}")
    logger.info(f"ðŸ”„ MERGE - Resultado: {merged}")

    return merged
